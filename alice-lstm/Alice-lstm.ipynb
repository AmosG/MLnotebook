{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:163815\n",
      "Total Vocab:60\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters:{}\".format(n_chars))\n",
    "print(\"Total Vocab:{}\".format(n_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:163715\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns:{}\".format(n_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "163715/163715 [==============================] - 495s 3ms/step - loss: 2.9515\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.95154, saving model to weights-improvement-01-2.9515-bigger.hdf5\n",
      "Epoch 2/50\n",
      "163715/163715 [==============================] - 498s 3ms/step - loss: 2.7516\n",
      "\n",
      "Epoch 00002: loss improved from 2.95154 to 2.75160, saving model to weights-improvement-02-2.7516-bigger.hdf5\n",
      "Epoch 3/50\n",
      "163715/163715 [==============================] - 499s 3ms/step - loss: 2.6534\n",
      "\n",
      "Epoch 00003: loss improved from 2.75160 to 2.65335, saving model to weights-improvement-03-2.6534-bigger.hdf5\n",
      "Epoch 4/50\n",
      "163715/163715 [==============================] - 499s 3ms/step - loss: 2.5753\n",
      "\n",
      "Epoch 00004: loss improved from 2.65335 to 2.57527, saving model to weights-improvement-04-2.5753-bigger.hdf5\n",
      "Epoch 5/50\n",
      "163715/163715 [==============================] - 500s 3ms/step - loss: 2.4993\n",
      "\n",
      "Epoch 00005: loss improved from 2.57527 to 2.49930, saving model to weights-improvement-05-2.4993-bigger.hdf5\n",
      "Epoch 6/50\n",
      "163715/163715 [==============================] - 499s 3ms/step - loss: 2.4321\n",
      "\n",
      "Epoch 00006: loss improved from 2.49930 to 2.43210, saving model to weights-improvement-06-2.4321-bigger.hdf5\n",
      "Epoch 7/50\n",
      "163715/163715 [==============================] - 500s 3ms/step - loss: 2.3744\n",
      "\n",
      "Epoch 00007: loss improved from 2.43210 to 2.37436, saving model to weights-improvement-07-2.3744-bigger.hdf5\n",
      "Epoch 8/50\n",
      "163715/163715 [==============================] - 499s 3ms/step - loss: 2.3211\n",
      "\n",
      "Epoch 00008: loss improved from 2.37436 to 2.32109, saving model to weights-improvement-08-2.3211-bigger.hdf5\n",
      "Epoch 9/50\n",
      "163715/163715 [==============================] - 499s 3ms/step - loss: 2.2723\n",
      "\n",
      "Epoch 00009: loss improved from 2.32109 to 2.27230, saving model to weights-improvement-09-2.2723-bigger.hdf5\n",
      "Epoch 10/50\n",
      "163715/163715 [==============================] - 500s 3ms/step - loss: 2.2269\n",
      "\n",
      "Epoch 00010: loss improved from 2.27230 to 2.22693, saving model to weights-improvement-10-2.2269-bigger.hdf5\n",
      "Epoch 11/50\n",
      "163715/163715 [==============================] - 500s 3ms/step - loss: 2.1867\n",
      "\n",
      "Epoch 00011: loss improved from 2.22693 to 2.18673, saving model to weights-improvement-11-2.1867-bigger.hdf5\n",
      "Epoch 12/50\n",
      "163715/163715 [==============================] - 499s 3ms/step - loss: 2.1457\n",
      "\n",
      "Epoch 00012: loss improved from 2.18673 to 2.14575, saving model to weights-improvement-12-2.1457-bigger.hdf5\n",
      "Epoch 13/50\n",
      "163715/163715 [==============================] - 500s 3ms/step - loss: 2.1110\n",
      "\n",
      "Epoch 00013: loss improved from 2.14575 to 2.11100, saving model to weights-improvement-13-2.1110-bigger.hdf5\n",
      "Epoch 14/50\n",
      "163715/163715 [==============================] - 500s 3ms/step - loss: 2.0813\n",
      "\n",
      "Epoch 00014: loss improved from 2.11100 to 2.08125, saving model to weights-improvement-14-2.0813-bigger.hdf5\n",
      "Epoch 15/50\n",
      "163715/163715 [==============================] - 500s 3ms/step - loss: 2.0493\n",
      "\n",
      "Epoch 00015: loss improved from 2.08125 to 2.04927, saving model to weights-improvement-15-2.0493-bigger.hdf5\n",
      "Epoch 16/50\n",
      "163715/163715 [==============================] - 500s 3ms/step - loss: 2.0216\n",
      "\n",
      "Epoch 00016: loss improved from 2.04927 to 2.02160, saving model to weights-improvement-16-2.0216-bigger.hdf5\n",
      "Epoch 17/50\n",
      "163715/163715 [==============================] - 500s 3ms/step - loss: 1.9946\n",
      "\n",
      "Epoch 00017: loss improved from 2.02160 to 1.99461, saving model to weights-improvement-17-1.9946-bigger.hdf5\n",
      "Epoch 18/50\n",
      "163715/163715 [==============================] - 502s 3ms/step - loss: 1.9687\n",
      "\n",
      "Epoch 00018: loss improved from 1.99461 to 1.96869, saving model to weights-improvement-18-1.9687-bigger.hdf5\n",
      "Epoch 19/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.9472\n",
      "\n",
      "Epoch 00019: loss improved from 1.96869 to 1.94717, saving model to weights-improvement-19-1.9472-bigger.hdf5\n",
      "Epoch 20/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.9285\n",
      "\n",
      "Epoch 00020: loss improved from 1.94717 to 1.92849, saving model to weights-improvement-20-1.9285-bigger.hdf5\n",
      "Epoch 21/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.9060\n",
      "\n",
      "Epoch 00021: loss improved from 1.92849 to 1.90601, saving model to weights-improvement-21-1.9060-bigger.hdf5\n",
      "Epoch 22/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.8931\n",
      "\n",
      "Epoch 00022: loss improved from 1.90601 to 1.89314, saving model to weights-improvement-22-1.8931-bigger.hdf5\n",
      "Epoch 23/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.8735\n",
      "\n",
      "Epoch 00023: loss improved from 1.89314 to 1.87352, saving model to weights-improvement-23-1.8735-bigger.hdf5\n",
      "Epoch 24/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.8592\n",
      "\n",
      "Epoch 00024: loss improved from 1.87352 to 1.85923, saving model to weights-improvement-24-1.8592-bigger.hdf5\n",
      "Epoch 25/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.8488\n",
      "\n",
      "Epoch 00025: loss improved from 1.85923 to 1.84876, saving model to weights-improvement-25-1.8488-bigger.hdf5\n",
      "Epoch 26/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.8314\n",
      "\n",
      "Epoch 00026: loss improved from 1.84876 to 1.83137, saving model to weights-improvement-26-1.8314-bigger.hdf5\n",
      "Epoch 27/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.8168\n",
      "\n",
      "Epoch 00027: loss improved from 1.83137 to 1.81677, saving model to weights-improvement-27-1.8168-bigger.hdf5\n",
      "Epoch 28/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.8121\n",
      "\n",
      "Epoch 00028: loss improved from 1.81677 to 1.81209, saving model to weights-improvement-28-1.8121-bigger.hdf5\n",
      "Epoch 29/50\n",
      "163715/163715 [==============================] - 505s 3ms/step - loss: 1.7923\n",
      "\n",
      "Epoch 00029: loss improved from 1.81209 to 1.79227, saving model to weights-improvement-29-1.7923-bigger.hdf5\n",
      "Epoch 30/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.7819\n",
      "\n",
      "Epoch 00030: loss improved from 1.79227 to 1.78193, saving model to weights-improvement-30-1.7819-bigger.hdf5\n",
      "Epoch 31/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.7756\n",
      "\n",
      "Epoch 00031: loss improved from 1.78193 to 1.77564, saving model to weights-improvement-31-1.7756-bigger.hdf5\n",
      "Epoch 32/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.7626\n",
      "\n",
      "Epoch 00032: loss improved from 1.77564 to 1.76261, saving model to weights-improvement-32-1.7626-bigger.hdf5\n",
      "Epoch 33/50\n",
      "163715/163715 [==============================] - 504s 3ms/step - loss: 1.7576\n",
      "\n",
      "Epoch 00033: loss improved from 1.76261 to 1.75755, saving model to weights-improvement-33-1.7576-bigger.hdf5\n",
      "Epoch 34/50\n",
      "163715/163715 [==============================] - 505s 3ms/step - loss: 1.7473\n",
      "\n",
      "Epoch 00034: loss improved from 1.75755 to 1.74733, saving model to weights-improvement-34-1.7473-bigger.hdf5\n",
      "Epoch 35/50\n",
      "163715/163715 [==============================] - 509s 3ms/step - loss: 1.7386\n",
      "\n",
      "Epoch 00035: loss improved from 1.74733 to 1.73856, saving model to weights-improvement-35-1.7386-bigger.hdf5\n",
      "Epoch 36/50\n",
      "163715/163715 [==============================] - 505s 3ms/step - loss: 1.7305\n",
      "\n",
      "Epoch 00036: loss improved from 1.73856 to 1.73047, saving model to weights-improvement-36-1.7305-bigger.hdf5\n",
      "Epoch 37/50\n",
      "163715/163715 [==============================] - 505s 3ms/step - loss: 1.7281\n",
      "\n",
      "Epoch 00037: loss improved from 1.73047 to 1.72806, saving model to weights-improvement-37-1.7281-bigger.hdf5\n",
      "Epoch 38/50\n",
      "163715/163715 [==============================] - 505s 3ms/step - loss: 1.7127\n",
      "\n",
      "Epoch 00038: loss improved from 1.72806 to 1.71272, saving model to weights-improvement-38-1.7127-bigger.hdf5\n",
      "Epoch 39/50\n",
      "163715/163715 [==============================] - 505s 3ms/step - loss: 1.7079\n",
      "\n",
      "Epoch 00039: loss improved from 1.71272 to 1.70794, saving model to weights-improvement-39-1.7079-bigger.hdf5\n",
      "Epoch 40/50\n",
      "163715/163715 [==============================] - 506s 3ms/step - loss: 1.6992\n",
      "\n",
      "Epoch 00040: loss improved from 1.70794 to 1.69925, saving model to weights-improvement-40-1.6992-bigger.hdf5\n",
      "Epoch 41/50\n",
      "163715/163715 [==============================] - 506s 3ms/step - loss: 1.6966\n",
      "\n",
      "Epoch 00041: loss improved from 1.69925 to 1.69663, saving model to weights-improvement-41-1.6966-bigger.hdf5\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163715/163715 [==============================] - 490s 3ms/step - loss: 1.6873\n",
      "\n",
      "Epoch 00042: loss improved from 1.69663 to 1.68730, saving model to weights-improvement-42-1.6873-bigger.hdf5\n",
      "Epoch 43/50\n",
      "163715/163715 [==============================] - 490s 3ms/step - loss: 1.6814\n",
      "\n",
      "Epoch 00043: loss improved from 1.68730 to 1.68136, saving model to weights-improvement-43-1.6814-bigger.hdf5\n",
      "Epoch 44/50\n",
      "163715/163715 [==============================] - 490s 3ms/step - loss: 1.6783\n",
      "\n",
      "Epoch 00044: loss improved from 1.68136 to 1.67826, saving model to weights-improvement-44-1.6783-bigger.hdf5\n",
      "Epoch 45/50\n",
      "163715/163715 [==============================] - 490s 3ms/step - loss: 1.6743\n",
      "\n",
      "Epoch 00045: loss improved from 1.67826 to 1.67429, saving model to weights-improvement-45-1.6743-bigger.hdf5\n",
      "Epoch 46/50\n",
      "163715/163715 [==============================] - 490s 3ms/step - loss: 1.6646\n",
      "\n",
      "Epoch 00046: loss improved from 1.67429 to 1.66460, saving model to weights-improvement-46-1.6646-bigger.hdf5\n",
      "Epoch 47/50\n",
      "163715/163715 [==============================] - 491s 3ms/step - loss: 1.6527\n",
      "\n",
      "Epoch 00047: loss improved from 1.66460 to 1.65271, saving model to weights-improvement-47-1.6527-bigger.hdf5\n",
      "Epoch 48/50\n",
      "163715/163715 [==============================] - 491s 3ms/step - loss: 1.6564\n",
      "\n",
      "Epoch 00048: loss did not improve from 1.65271\n",
      "Epoch 49/50\n",
      "163715/163715 [==============================] - 491s 3ms/step - loss: 1.6481\n",
      "\n",
      "Epoch 00049: loss improved from 1.65271 to 1.64806, saving model to weights-improvement-49-1.6481-bigger.hdf5\n",
      "Epoch 50/50\n",
      "163715/163715 [==============================] - 501s 3ms/step - loss: 1.6426\n",
      "\n",
      "Epoch 00050: loss improved from 1.64806 to 1.64264, saving model to weights-improvement-50-1.6426-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6bdeb1a518>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "#model.add(LSTM(256))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-50-1.6426-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110725\n",
      "[32, 34, 48, 1, 38, 43, 1, 49, 37, 34, 38, 47, 1, 45, 30, 52, 48, 11, 0, 0, 56, 30, 43, 33, 1, 37, 44, 52, 1, 42, 30, 43, 54, 1, 37, 44, 50, 47, 48, 1, 30, 1, 33, 30, 54, 1, 33, 38, 33, 1, 54, 44, 50, 1, 33, 44, 1, 41, 34, 48, 48, 44, 43, 48, 25, 57, 1, 48, 30, 38, 33, 1, 30, 41, 38, 32, 34, 9, 1, 38, 43, 1, 30, 1, 37, 50, 47, 47, 54, 1, 49, 44, 0, 32, 37, 30, 43, 36, 34, 1]\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print(start)\n",
    "print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the pupjrstio.\n",
      "\n",
      "‘io yhu the pabt of think!’ thi ganter want, tiriing to aeiin igaiing. \n",
      "‘whal i can’t be a mett ro the teal to the pemt oite!’ said the kanter. \n",
      "‘i dene tou donn the roett suam ’hur mooe,’ said the mouge, \n",
      "‘ie toued io the mert of thet ar the saal then i mo,’ said the kante rante ‘once. ‘iov aelind to ba ii  mh dirrsr,’ \n",
      "‘h don’t know what i sool toe mioe than ’ said the monke, ‘i most en would ie wound be ko dltttr the madt ofde io the dane,-\n",
      "the docm shen shen she wan sointing oo to the that, and the white rabbit inee aoond the was so tie thame whth their siaecs, and the thou ht was ooe of them with tie wilne the was soitting to be in a lotte toide, ‘hhn the luoy be a ba moce in the listle bu the whitew as the luosr, and the waited tf ten to the merthoe oo the taaeet to sey to hir toapes, and whnt hir toict to sie kant, \n",
      "‘the was toent it was in an a latter on the dirtt benue the white!’ she shiught th herself, ‘i don’t think to be is ailind to tey the maac of theng w"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
